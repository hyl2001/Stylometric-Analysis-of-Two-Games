import pandas as pd
import numpy as np
import math

from typing import Literal
from glob import glob
from os.path import join
from json import load


# NOTE: Unless a special statement is made, fomulars used are from [1].
# Reference: [1] 刘海涛. 2017, 计量语言学导论[M]. 北京: 商务印书馆.
class GetIndecators:
    def __init__(self, 
                 *,
                 freq_scale:float=1
                ) -> None:
        self.freq_scale = freq_scale

        # Some global variables are defined in methods, shown as below.
        # self.wrd_lemma
        # self.counter
        # self.content
        # self.wrd_rank_freq_dis
        # self.L_val
        # self.gini_val
    
    def __save_div(self, a, b):
        if b == 0:
            return 0
        else:
            return a / b
    
    def __get_lemma_pos(self, tagged_txt:str):
        el_list = tagged_txt.strip().split('_')
        if len(el_list) != 2:
            # Some file may incorrectly lemmatized (for example, 通_过_P),
            # here, I split the text by '_'  and take the last one in the list
            # generated by .split() fuction as POS-tag. The rest are combined
            # with '_'.
            pos = el_list[-1]
            char_ = '_'.join(el_list[:-1])
    
            return char_, pos
        elif len(el_list) == 2:
            return tuple(el_list)
        else:
            raise ValueError('A mul-formed tagged_txt is input.')
    
    def __counter(self, lemma_pos_lst:list[str]):
        counter = {}

        for lemma_pos in lemma_pos_lst:
            lemma, pos = self.__get_lemma_pos(lemma_pos)

            if lemma not in counter:
                counter[lemma] = {'occurance': 0}
            
            if pos not in counter[lemma]:
                counter[lemma][pos] = 0
            
            counter[lemma][pos] += 1
            counter[lemma]['occurance'] += 1
        
        self.counter_var = pd.DataFrame(counter).T
        return self.counter_var  

    def __load_file(self, file_path:str):
        with open(file_path, 'r', encoding='utf-8') as op:
            self.content_var = op.readlines()

            self.lemma_pos_var = \
                [i.strip() 
                 for i in self.content_var 
                 if i.strip().split('_')[-1] != 'PU']
            self.lemma_pos_var = \
                list(map(self.__get_lemma_pos, self.lemma_pos_var))

    def get_wrd_rank_freq_dis(self) -> pd.DataFrame:    
        if len(self.content_var) == 0:
            raise ValueError('Nihil length of file.')

        counter = self.__counter(self.content_var)
        # https://datascience.stackexchange.com/questions/53676/how-to-delete-a-row-if-a-values-in-a-column-is-not-nan
        counter = counter[pd.isna(counter['PU'])]
        df_freq = counter['occurance'].sort_values(ascending=False).to_frame()
        n_words = df_freq.sum()

        df_freq['rank'] = range(1, len(df_freq.index)+1)
        df_freq['freq'] = df_freq['occurance'].apply(lambda x: x * self.freq_scale / n_words)

        self.wrd_rank_freq_dis_var = df_freq
        
        return df_freq
    
    def h_point(self):
        rank = self.wrd_rank_freq_dis_var['rank']
        occurance = self.wrd_rank_freq_dis_var['occurance']

        r_lower_than_fr_cond_series = rank < occurance

        r1 = rank[r_lower_than_fr_cond_series].iloc[-1]
        f1 = occurance[r_lower_than_fr_cond_series].iloc[-1]
        r_greater_than_fr_cond_series = rank >= occurance
        r2 = rank[r_greater_than_fr_cond_series].iloc[0]
        f2 = occurance[r_greater_than_fr_cond_series].iloc[0]
          
        h = (f1 * r2 - f2 * r1) / (r2 - r1 + f1 - f2)
        self.h_var = h
         
        return h
    
    def entropy(self):
        val = self.wrd_rank_freq_dis_var['freq'].apply(lambda x: x * math.log2(x))
        
        return -np.sum(val)
    
    def avg_tokens_len(self):
        return np.mean([len(i[0]) for i in self.lemma_pos_var if i[-1] != 'PU'])
    
    def R1(self):
        try: 
            h = self.h_var
        except AttributeError:
            print('Warning: cannot find self.h; run self.h_point().')
            h = self.h_point()

        fh = self.wrd_rank_freq_dis_var[:int(h)]['freq'].sum()
        adj_fh = fh - (h ** 2 / (2 * len(self.lemma_pos_var)))

        return 1 - adj_fh
    
    def repeat_rate(self):
        self.rr_var = np.sum(np.square(self.wrd_rank_freq_dis_var['freq'].to_list()))
        
        return self.rr_var
    
    def relative_repeat_rate(self):
        try:
            return (1 - np.sqrt(self.rr_var)) / 1 - (np.power(np.sqrt(len(self.content_var)), -1))
        except AttributeError:
            # raise NameError('Method repeat_rate() should be run before calling this method.')
            self.repeat_rate()

            return (1 - np.sqrt(self.rr_var)) / 1 - (np.power(np.sqrt(len(self.content_var), -1)))
    
    def __truncate_by_pos(self, by:Literal['content', 'functional'], h:float):
        '''
        POS tags of content words:
            noun (NN, NR, FW)
                time noun (NT)
                location noun(处所词) *
            verb (VC, VE, VV)
            adjective (VA)
            pronoun (PN, DT)
            numeral (CD, OD)
            classifier/quantifier(数词) (M)
            localizer (LC)
            attributive(区别词) (JJ)
        * Note that location nouns are classified as nouns in CTB POS tagging scheme.

        POS tags of functional words:
            preposition (MSP, LC, P, LB, SB)
            auxiliary(助词) (AS, DEC, DEG, DEV, DER)
            conjunction (CC, CS)
            adverb (AD)
            modal particle(语气词) (IJ, ON, SP)

        ** All listed above are cited from [1] pp.40 quoted from [2] pp.126.

        References:
        [1] 朱德熙. 《语法讲义》. 北京: 商务印书馆z. 1982.
        [2] 刘颖 编著. 《统计语言学》. 北京: 清华大学出版社. 2014.
        '''

        content_word_pos = [
            'NN', 'NR', 'NT', 'FW', 'VC', 'VE', 'VV', 'VA', 'PN', 
            'DT', 'CD', 'OD', 'M', 'LC', 'JJ', 
            'rank' # For the convinence of further calculation, 'rank' is reserved.
        ]
        functional_word_pos = [
            'MSP', 'LC', 'P', 'LB', 'SB', 'AS', 'DEC', 'DEG', 
            'DEV', 'DER', 'CC', 'CS', 'AD', 'IJ', 'ON', 'SP',
            'rank' # For the convinence of further calculation, 'rank' is reserved.
        ]

        if h == 0:
            raise ValueError('"h" cannot be zero.')

        counter_sorted = self.counter_var.sort_values('occurance', ascending=False)
        # This can remove punctuations from the counter.
        counter_sorted = counter_sorted[pd.isna(counter_sorted['PU'])]
        counter_sorted['rank'] = [i for i in range(1, len(counter_sorted.index)+1)]
        
        clipped_freq_table = \
            counter_sorted.head(int(2*h))

        # https://stackoverflow.com/questions/70098789/how-to-ignore-not-in-index-error-in-python
        if by == 'content':
            selected_freq_table = \
                clipped_freq_table[clipped_freq_table.columns.intersection(content_word_pos)]
        elif by == 'functional':
            selected_freq_table = \
                clipped_freq_table[clipped_freq_table.columns.intersection(functional_word_pos)]
        else:
            raise ValueError('"by" cannot take parameter other than "content" and "functional".')

        f1 = clipped_freq_table.iloc[0]['occurance']

        return selected_freq_table, f1
        
    def __avg_rank(self, content_wrd_freq_table:pd.DataFrame):
        copied = content_wrd_freq_table.copy()
        grouped = copied.groupby('sum')
        avg_rank_of_each_freq = grouped.mean()

        for freq, avg_rank in avg_rank_of_each_freq.itertuples():
            copied.loc[copied['sum'] == freq, 'avg_rank'] = avg_rank

        return copied
    
    def __thematic_concent(self, 
                           full_wrd_freq_table:pd.DataFrame, 
                           h:float, 
                           f1:float, 
                           kind:Literal['h', '2h']='h') -> float:
        # the 'factor' defined below is the coefficience before h in 
        # formular (6.37) and (6.42) of [1].
        # Reference:
        # [1] Kubát, Miroslav & Matlach, Vladimír & Čech, Radek. (2014). 
        # QUITA – Quantitative Index Text Analyzer. 
        if kind == 'h':
            n_per_wrd = full_wrd_freq_table[:int(h)]
            factor = 1
        elif kind == '2h':
            n_per_wrd = full_wrd_freq_table[:int(2*h)]
            factor = 2
        else:
            raise ValueError('"kind" should either be "h" or "2h".')
        
        n_per_wrd_sum = n_per_wrd.T.sum()
        
        # sum() will sum the column of 'rank' while calculation.
        # By subtrack the sum with n_per_wrd['rank'],
        # the extra values can be eliminated (see following code).
        n_per_wrd_sum = n_per_wrd_sum - n_per_wrd['rank']
        
        # concatenate n_per_wrd_sum with original rank (n_per_wrd['rank'])
        # for facilitating further steps.
        n_per_wrd_rank = pd.concat([n_per_wrd_sum, n_per_wrd['rank']], axis='columns')
        
        # Rename the sum value column (the very name is 0 in int) with 'sum'.
        n_per_wrd_sum = n_per_wrd_rank.rename({0: 'sum'}, axis='columns')

        if kind == '2h':
            # This was created for the reason that can be found in [1].
            # Reference:
            # [1] 陈蕊娜.基于计量语言学指标的汉英文本特征比较研究[D].
            # 浙江大学, 2017.DOI:10.27461/d.cnki.gzjdx.2017.000033.
            n_per_content_wrd = self.__avg_rank(n_per_wrd_sum)
        
        # The truncate_by_pos() will remain all words in the original DataFrame (self.counter)
        # instead of what we want by filling their counts with nan. To get the words we request,
        # we can rule them out by removing the words with zero counts.
        n_per_content_wrd = n_per_wrd_sum[n_per_wrd_sum['sum'] != 0]
        # print(n_per_content_wrd)
        
        # The denominator of the formular (3-17) in [1] pp. 136.
        # Reference: [1] 刘海涛. 2017, 计量语言学导论[M]. 北京: 商务印书馆.
        denominator_int = h * (factor * h - 1) * f1
        
        # Subsequent codes use vector operation.
        h_sub_r_vec = factor * h - n_per_content_wrd['rank']
        numerator_vec = h_sub_r_vec * n_per_content_wrd['sum']
        
        return np.sum(numerator_vec / denominator_int) # thematic concentration
    
    def thematic_concent(self):
        try: 
            h = self.h_var
        except AttributeError:
            print('Warning: cannot find self.h; run self.h_point().')
            h = self.h_point()

        n_per_wrd, f1 = self.__truncate_by_pos('content', h)

        return self.__thematic_concent(n_per_wrd, h, f1, 'h')

    def sec_tc(self):
        try: 
            h = self.h_var
        except AttributeError:
            print('Warning: cannot find self.h; run self.h_point().')
            h = self.h_point()
        
        n_per_wrd, f1 = self.__truncate_by_pos('content', h)

        return self.__thematic_concent(n_per_wrd, h, f1, '2h')
    
    def acitivity(self):
        verb_pos = ['VC', 'VE', 'VV']
        
        n_verb = 0
        n_word = 0

        for _, lemma in self.lemma_pos_var:
            if lemma in verb_pos:
                n_verb += 1
            n_word += 1
        
        return self.__save_div(n_verb, n_word)

    def descirptivity(self):
        adj_verb_pos = ['VA', 'JJ', 'VC', 'VE', 'VV']
         
        n_adj_verb = 0
        n_word = 0
        
        for _, lemma in self.lemma_pos_var:
            if lemma in adj_verb_pos:
                n_adj_verb += 1
            n_word += 1
         
        return self.__save_div(n_adj_verb, n_word)
    
    def __L(self, kind:Literal['h', 'all'] = 'all'):
        if kind == 'h':
            end_pos = int(self.h_var)
        elif kind == 'all':
            end_pos = len(self.wrd_rank_freq_dis_var)
        else:
            raise ValueError('"kind" should be either "h" or "all".')
        
        calc = lambda x, y: math.sqrt((x - y) ** 2 + 1)

        single_values_list = [
            calc(self.wrd_rank_freq_dis_var.iloc[idx]['occurance'], 
                 self.wrd_rank_freq_dis_var.iloc[idx+1]['occurance']) 
            for idx in range(0, end_pos-1)]
        
        return np.sum(single_values_list)

    def L(self):
        self.L_var = self.__L()

        return self.L_var 
    
    def curve_length_L_index(self):
        try:
            l = self.L_var
        except AttributeError:
            print('Warning: cannot find self.L_val; run self.L().')
            l = self.L()

        return 1 - (self.__L('h') / l)
    
    def txt_lambda(self):
        try:
            l = self.L_var
        except AttributeError:
            print('Warning: cannot find self.L_val; run self.L().')
            l = self.L()

        return \
            (l * math.log10(len(self.lemma_pos_var))) / len(self.lemma_pos_var)
    
    def adj_modulous(self):
        counter_sorted = self.counter_var.sort_values('occurance', ascending=False)
        counter_sorted = counter_sorted[pd.isna(counter_sorted['PU'])]

        f1 = counter_sorted.iloc[0]['occurance'].sum()

        try: 
            h = self.h_var
        except AttributeError:
            print('Warning: cannot find self.h; run self.h_point().')
            h = self.h_point()
        
        m = math.sqrt((f1 / h) ** 2 + (len(self.wrd_rank_freq_dis_var.index) / h) ** 2)

        return m / math.log10(len(self.lemma_pos_var))
    
    def gini(self):
        r_times_fr = self.wrd_rank_freq_dis_var['occurance'] * self.wrd_rank_freq_dis_var['rank']
        sum_r_times_fr = r_times_fr.sum()
        _2_avg_r_times_fr = 2 * (sum_r_times_fr) / len(self.lemma_pos_var)

        n_types = self.wrd_rank_freq_dis_var['occurance'].sum()

        self.gini_var = (n_types + 1 - _2_avg_r_times_fr) / n_types

        return self.gini_var
    
    def R4(self):
        try: 
            gini_val = self.gini_var
        except AttributeError:
            print('Warning: cannot find self.gini_val; run self.gini().')
            gini_val = self.gini()
        
        return 1 - gini_val
    
    def hapax_perc(self):
        n_hapax = self.wrd_rank_freq_dis_var[self.wrd_rank_freq_dis_var['occurance'] == 1]

        return n_hapax.shape[0] / len(self.lemma_pos_var)
    
    def writers_view(self):
        # variables naming:
        #   a = -[(h-1)(f1-h) + (h+1)(V-h)]
        #   b = sqrt( (h-1)**2 + (f1-h)**2 )
        #   c = sqrt( (h-1)**2 + (V-h)**2 )
        #   cos_alpha = a / (b * c)
        # Note that this formular is from [1] pp. 110 formular (6.34) and
        # [2] pp. 138 formular (3-26).
        #
        # References:
        # [1]Kubát, Miroslav & Matlach, Vladimír & Čech, Radek. (2014). 
        # QUITA – Quantitative Index Text Analyzer. 
        # [2] 刘海涛. 2017, 计量语言学导论[M]. 北京: 商务印书馆.
        
        try: 
            h = self.h_var
        except AttributeError:
            print('Warning: cannot find self.h; run self.h_point().')
            h = self.h_point()
        
        counter_sorted = self.counter_var.sort_values('occurance', ascending=False)
        counter_sorted = counter_sorted[pd.isna(counter_sorted['PU'])]
        
        f1 = counter_sorted.iloc[0]['occurance'].sum()
        V = self.wrd_rank_freq_dis_var['occurance'].sum() # n_types

        a = -((h-1) * (f1-h) + (h+1) * (V-h))
        b = math.sqrt( (h-1)**2 + (f1-h)**2 )
        c = math.sqrt( (h-1)**2 + (V-h)**2 )

        return a / (b * c)
    
    def verb_dist(self):
        verb_pos = ['VC', 'VE', 'VV']
         
        dists_list = []
        dist = 0
        
        for _, lemma in self.lemma_pos_var:
            if lemma in verb_pos:
                dists_list.append(dist)
                dist = -1

            dist += 1

        return np.mean(dists_list)

    def run(self, filepath:str, idx_name:str|None = None):
        self.__load_file(filepath)
        self.get_wrd_rank_freq_dis()

        params = {
            'h-point': self.h_point(),
            'entropy': self.entropy(),
            'avg_tokens_len': self.avg_tokens_len(),
            'R1': self.R1(),
            'repeat_rate': self.repeat_rate(),
            'relative_repeat_rate': self.relative_repeat_rate(),
            'thematice_concentration': self.thematic_concent(),
            'secondary_thematic_concentration': self.sec_tc(),
            'activity': self.acitivity(),
            'descriptivity': self.descirptivity(),
            'L': self.L(),
            'curve_length_L_index': self.curve_length_L_index(),
            'txt_lambda': self.txt_lambda(),
            'adjusted_modulous': self.adj_modulous(),
            'gini_coef': self.gini(),
            'R4': self.R4(),
            'hapax_percentage': self.hapax_perc(),
            'writers_view': self.writers_view(),
            'verb_distance': self.verb_dist()
        }

        lst = [el for el in dir(self) if el.endswith('_var')]

        for var in lst:
            delattr(self, var)

        name = idx_name if idx_name else filepath.split('\\')[0]

        return pd.DataFrame(params, index=[name])
    
    def run_on_folder(self, nlpfolderpath:str, 
                      files_included_json_path:str, 
                      save_as_xlsx:str|None=None):
        with open(files_included_json_path, 'r', encoding='utf-8') as f:
            files_included = load(f)
        
        res = []

        for file in glob(join(nlpfolderpath, '*')):
            filename = file.split('\\')[-1]
            if filename in files_included:
                print(filename)
                res.append(self.run(file, filename))
        
        df_res = pd.concat(res)
        if save_as_xlsx:
            df_res.to_excel(save_as_xlsx)
        
        return df_res
